{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-TDDFT ML Model Results Analysis\n",
    "\n",
    "This notebook provides tools for analyzing trained RT-TDDFT ML models:\n",
    "\n",
    "1. **Training Analysis** - Loss curves, learning rate, curriculum progress\n",
    "2. **Model Evaluation** - Metrics on test trajectories\n",
    "3. **Error Analysis** - Error accumulation over rollout steps\n",
    "4. **Physics Constraints** - Trace, Hermiticity, idempotency violations\n",
    "5. **Spectral Analysis** - Absorption spectrum comparison\n",
    "6. **Density Visualization** - Predicted vs true density matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.utils import (\n",
    "    # Metrics\n",
    "    compute_trajectory_metrics,\n",
    "    compute_step_errors,\n",
    "    compute_absorption_spectrum,\n",
    "    spectrum_overlap,\n",
    "    MetricsAccumulator,\n",
    "    # Visualization\n",
    "    plot_training_curves,\n",
    "    plot_loss_components,\n",
    "    plot_error_accumulation,\n",
    "    plot_physics_violations,\n",
    "    plot_absorption_spectrum,\n",
    "    plot_density_matrix,\n",
    "    plot_density_comparison,\n",
    "    plot_dipole_trajectory,\n",
    "    plot_metrics_comparison,\n",
    "    plot_curriculum_progress,\n",
    "    create_training_dashboard,\n",
    "    close_all,\n",
    ")\n",
    "from src.inference import Predictor, RolloutConfig\n",
    "from src.data import Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set paths to your checkpoint and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - modify these paths\n",
    "CHECKPOINT_PATH = \"../checkpoints/phase1_h2p/best.pt\"  # Path to trained model\n",
    "DATA_PATH = \"../data/processed\"  # Path to test data\n",
    "MOLECULE = \"h2\"  # Molecule to analyze\n",
    "\n",
    "# Analysis settings\n",
    "MAX_TRAJECTORIES = 10  # Number of trajectories to analyze\n",
    "MAX_ROLLOUT_STEPS = 100  # Maximum rollout steps\n",
    "DT = 0.1  # Time step in atomic units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Training Analysis\n",
    "\n",
    "Load and visualize training history from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_history(checkpoint_path):\n",
    "    \"\"\"Load training history from checkpoint.\"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': checkpoint.get('train_losses', []),\n",
    "        'val_losses': checkpoint.get('val_losses', []),\n",
    "        'epoch': checkpoint.get('epoch', 0),\n",
    "        'global_step': checkpoint.get('global_step', 0),\n",
    "        'best_val_loss': checkpoint.get('best_val_loss', None),\n",
    "    }\n",
    "    \n",
    "    # Try to load loss components if available\n",
    "    if 'loss_history' in checkpoint:\n",
    "        history['loss_components'] = checkpoint['loss_history']\n",
    "    \n",
    "    # Try to load learning rate history\n",
    "    if 'lr_history' in checkpoint:\n",
    "        history['lr_history'] = checkpoint['lr_history']\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "history = load_training_history(CHECKPOINT_PATH)\n",
    "\n",
    "if history:\n",
    "    print(f\"Epochs trained: {history['epoch']}\")\n",
    "    print(f\"Global steps: {history['global_step']}\")\n",
    "    print(f\"Best validation loss: {history['best_val_loss']}\")\n",
    "    print(f\"Final training loss: {history['train_losses'][-1] if history['train_losses'] else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if history and history['train_losses']:\n",
    "    fig = plot_training_curves(\n",
    "        train_losses=history['train_losses'],\n",
    "        val_losses=history.get('val_losses'),\n",
    "        title=\"Training Progress\",\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss components if available\n",
    "if history and 'loss_components' in history:\n",
    "    fig = plot_loss_components(\n",
    "        loss_history=history['loss_components'],\n",
    "        title=\"Loss Components Over Training\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dashboard (if we have enough data)\n",
    "if history and history['train_losses']:\n",
    "    fig = create_training_dashboard(\n",
    "        train_losses=history['train_losses'],\n",
    "        val_losses=history.get('val_losses'),\n",
    "        loss_components=history.get('loss_components'),\n",
    "        lr_history=history.get('lr_history'),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, device='cpu'):\n",
    "    \"\"\"Load model from checkpoint.\"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    if 'model' in checkpoint:\n",
    "        model = checkpoint['model']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        raise ValueError(\"Checkpoint contains only state_dict. Need full model.\")\n",
    "    else:\n",
    "        model = checkpoint\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trajectories(data_path, molecule=None, max_trajectories=None):\n",
    "    \"\"\"Load trajectory data from HDF5 files.\"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    trajectories = []\n",
    "    \n",
    "    # Find HDF5 files\n",
    "    if molecule:\n",
    "        patterns = [f\"*{molecule}*.h5\", f\"*{molecule}*.hdf5\"]\n",
    "    else:\n",
    "        patterns = [\"*.h5\", \"*.hdf5\"]\n",
    "    \n",
    "    files = []\n",
    "    for pattern in patterns:\n",
    "        files.extend(data_path.glob(pattern))\n",
    "    \n",
    "    if max_trajectories:\n",
    "        files = files[:max_trajectories]\n",
    "    \n",
    "    print(f\"Found {len(files)} trajectory files\")\n",
    "    \n",
    "    for f in files:\n",
    "        try:\n",
    "            traj = Trajectory.load(f)\n",
    "            trajectories.append(traj)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {f}: {e}\")\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model(CHECKPOINT_PATH, device=device)\n",
    "\n",
    "if model:\n",
    "    print(f\"Model loaded successfully\")\n",
    "    # Print model info if available\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test trajectories\n",
    "trajectories = load_trajectories(DATA_PATH, molecule=MOLECULE, max_trajectories=MAX_TRAJECTORIES)\n",
    "print(f\"Loaded {len(trajectories)} trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Evaluation\n",
    "\n",
    "Evaluate model on test trajectories and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trajectory(model, trajectory, max_steps=None, device='cpu'):\n",
    "    \"\"\"Evaluate model on a single trajectory.\"\"\"\n",
    "    # Convert to tensors\n",
    "    densities = torch.tensor(trajectory.densities, device=device, dtype=torch.complex64)\n",
    "    fields = torch.tensor(trajectory.fields, device=device, dtype=torch.float32)\n",
    "    overlap = torch.tensor(trajectory.overlap, device=device, dtype=torch.complex64)\n",
    "    n_electrons = trajectory.n_electrons\n",
    "    \n",
    "    geometry = {\n",
    "        'positions': torch.tensor(trajectory.positions, device=device, dtype=torch.float32),\n",
    "        'atomic_numbers': torch.tensor(trajectory.atomic_numbers, device=device),\n",
    "    }\n",
    "    \n",
    "    # Get dipole integrals if available\n",
    "    dipole_integrals = None\n",
    "    if hasattr(trajectory, 'dipole_integrals') and trajectory.dipole_integrals is not None:\n",
    "        dipole_integrals = torch.tensor(\n",
    "            trajectory.dipole_integrals, device=device, dtype=torch.complex64\n",
    "        )\n",
    "    \n",
    "    # Setup predictor\n",
    "    predictor = Predictor(model)\n",
    "    n_steps = max_steps or (len(densities) - 1)\n",
    "    n_steps = min(n_steps, len(densities) - 1)\n",
    "    \n",
    "    config = RolloutConfig(\n",
    "        max_steps=n_steps,\n",
    "        apply_physics_projection=False,\n",
    "    )\n",
    "    \n",
    "    # Run rollout\n",
    "    with torch.no_grad():\n",
    "        result = predictor.rollout(\n",
    "            initial_density=densities[0],\n",
    "            geometry=geometry,\n",
    "            field_sequence=fields[:n_steps + 1],\n",
    "            overlap=overlap,\n",
    "            n_electrons=n_electrons,\n",
    "            config=config,\n",
    "        )\n",
    "    \n",
    "    pred = result.densities\n",
    "    true = densities[1:n_steps + 1]\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(pred), len(true))\n",
    "    pred = pred[:min_len]\n",
    "    true = true[:min_len]\n",
    "    \n",
    "    return {\n",
    "        'pred': pred,\n",
    "        'true': true,\n",
    "        'overlap': overlap,\n",
    "        'n_electrons': n_electrons,\n",
    "        'dipole_integrals': dipole_integrals,\n",
    "        'fields': fields[:min_len + 1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all trajectories\n",
    "results = []\n",
    "accumulator = MetricsAccumulator()\n",
    "\n",
    "if model and trajectories:\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        print(f\"Evaluating trajectory {i+1}/{len(trajectories)}...\", end='\\r')\n",
    "        \n",
    "        eval_result = evaluate_trajectory(\n",
    "            model, traj, max_steps=MAX_ROLLOUT_STEPS, device=device\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_trajectory_metrics(\n",
    "            trajectory_pred=eval_result['pred'],\n",
    "            trajectory_true=eval_result['true'],\n",
    "            overlap=eval_result['overlap'],\n",
    "            n_electrons=eval_result['n_electrons'],\n",
    "            dipole_integrals=eval_result['dipole_integrals'],\n",
    "            dt=DT,\n",
    "        )\n",
    "        \n",
    "        eval_result['metrics'] = metrics\n",
    "        results.append(eval_result)\n",
    "        \n",
    "        accumulator.add({\n",
    "            'mse': metrics.mse,\n",
    "            'mae': metrics.mae,\n",
    "            'relative_error': metrics.relative_error,\n",
    "            'max_error': metrics.max_error,\n",
    "            'trace_violation': metrics.trace_violation,\n",
    "            'hermiticity_violation': metrics.hermiticity_violation,\n",
    "            'dipole_error': metrics.dipole_error,\n",
    "            'spectrum_overlap': metrics.spectrum_overlap,\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nEvaluated {len(results)} trajectories\")\n",
    "else:\n",
    "    print(\"Model or trajectories not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "if results:\n",
    "    summary = accumulator.compute_summary()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metric, stats in summary.items():\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  mean:   {stats['mean']:.6e}\")\n",
    "        print(f\"  std:    {stats['std']:.6e}\")\n",
    "        print(f\"  min:    {stats['min']:.6e}\")\n",
    "        print(f\"  max:    {stats['max']:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Error Accumulation Analysis\n",
    "\n",
    "Analyze how errors accumulate over rollout steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute step-by-step errors for first trajectory\n",
    "if results:\n",
    "    result = results[0]\n",
    "    step_errors = compute_step_errors(result['pred'], result['true'])\n",
    "    \n",
    "    print(f\"Trajectory length: {len(result['pred'])} steps\")\n",
    "    print(f\"Initial step MSE: {step_errors['step_mse'][0]:.6e}\")\n",
    "    print(f\"Final step MSE: {step_errors['step_mse'][-1]:.6e}\")\n",
    "    print(f\"Error growth factor: {step_errors['step_mse'][-1] / step_errors['step_mse'][0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error accumulation\n",
    "if results:\n",
    "    fig = plot_error_accumulation(\n",
    "        step_errors=step_errors['step_relative_error'],\n",
    "        cumulative_errors=step_errors['cumulative_error'],\n",
    "        dt=DT,\n",
    "        title=\"Error Accumulation Over Rollout\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average error accumulation across all trajectories\n",
    "if len(results) > 1:\n",
    "    all_step_errors = []\n",
    "    min_len = min(len(r['pred']) for r in results)\n",
    "    \n",
    "    for r in results:\n",
    "        errors = compute_step_errors(r['pred'][:min_len], r['true'][:min_len])\n",
    "        all_step_errors.append(errors['step_relative_error'].cpu().numpy())\n",
    "    \n",
    "    all_step_errors = np.array(all_step_errors)\n",
    "    mean_errors = all_step_errors.mean(axis=0)\n",
    "    std_errors = all_step_errors.std(axis=0)\n",
    "    \n",
    "    # Plot with error bands\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(min_len) * DT\n",
    "    plt.plot(x, mean_errors, label='Mean Error', color='tab:blue')\n",
    "    plt.fill_between(x, mean_errors - std_errors, mean_errors + std_errors, \n",
    "                     alpha=0.3, color='tab:blue', label='Std Dev')\n",
    "    plt.xlabel('Time (a.u.)')\n",
    "    plt.ylabel('Relative Error')\n",
    "    plt.title('Average Error Accumulation Across Trajectories')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Physics Constraint Analysis\n",
    "\n",
    "Analyze how well the model preserves physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.metrics import trace_violation, hermiticity_violation, idempotency_violation\n",
    "\n",
    "def compute_physics_violations(pred_trajectory, overlap, n_electrons):\n",
    "    \"\"\"Compute physics violations at each step.\"\"\"\n",
    "    trace_viol = trace_violation(pred_trajectory, overlap, n_electrons)\n",
    "    herm_viol = hermiticity_violation(pred_trajectory)\n",
    "    idem_viol = idempotency_violation(pred_trajectory, overlap)\n",
    "    \n",
    "    return {\n",
    "        'trace': trace_viol.cpu().numpy(),\n",
    "        'hermiticity': herm_viol.cpu().numpy(),\n",
    "        'idempotency': idem_viol.cpu().numpy(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute physics violations for first trajectory\n",
    "if results:\n",
    "    result = results[0]\n",
    "    violations = compute_physics_violations(\n",
    "        result['pred'], result['overlap'], result['n_electrons']\n",
    "    )\n",
    "    \n",
    "    print(f\"Trace violation - mean: {violations['trace'].mean():.6e}, max: {violations['trace'].max():.6e}\")\n",
    "    print(f\"Hermiticity violation - mean: {violations['hermiticity'].mean():.6e}, max: {violations['hermiticity'].max():.6e}\")\n",
    "    print(f\"Idempotency violation - mean: {violations['idempotency'].mean():.6e}, max: {violations['idempotency'].max():.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot physics violations\n",
    "if results:\n",
    "    fig = plot_physics_violations(\n",
    "        trace_violations=violations['trace'],\n",
    "        hermiticity_violations=violations['hermiticity'],\n",
    "        idempotency_violations=violations['idempotency'],\n",
    "        dt=DT,\n",
    "        title=\"Physics Constraint Violations\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Spectral Analysis\n",
    "\n",
    "Compare predicted and true absorption spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dipole_trajectory(densities, dipole_integrals):\n",
    "    \"\"\"Compute dipole moments from density trajectory.\"\"\"\n",
    "    return torch.einsum(\"tij,cji->tc\", densities, dipole_integrals).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and compare spectra\n",
    "if results and results[0]['dipole_integrals'] is not None:\n",
    "    result = results[0]\n",
    "    \n",
    "    # Compute dipole trajectories\n",
    "    dipole_pred = compute_dipole_trajectory(result['pred'], result['dipole_integrals'])\n",
    "    dipole_true = compute_dipole_trajectory(result['true'], result['dipole_integrals'])\n",
    "    \n",
    "    # Compute absorption spectra\n",
    "    freqs, spec_pred = compute_absorption_spectrum(dipole_pred, DT)\n",
    "    _, spec_true = compute_absorption_spectrum(dipole_true, DT)\n",
    "    \n",
    "    # Compute overlap\n",
    "    overlap_val = spectrum_overlap(spec_pred, spec_true, freqs, freq_range=(0, 20))\n",
    "    print(f\"Spectrum overlap: {overlap_val:.4f}\")\n",
    "else:\n",
    "    print(\"Dipole integrals not available for spectral analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spectrum comparison\n",
    "if results and results[0]['dipole_integrals'] is not None:\n",
    "    fig = plot_absorption_spectrum(\n",
    "        freqs=freqs,\n",
    "        spectrum_pred=spec_pred,\n",
    "        spectrum_true=spec_true,\n",
    "        freq_range=(0, 20),\n",
    "        title=f\"Absorption Spectrum (Overlap: {overlap_val:.4f})\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dipole moment trajectories\n",
    "if results and results[0]['dipole_integrals'] is not None:\n",
    "    fig = plot_dipole_trajectory(\n",
    "        dipole_pred=dipole_pred.cpu().numpy(),\n",
    "        dipole_true=dipole_true.cpu().numpy(),\n",
    "        dt=DT,\n",
    "        title=\"Dipole Moment Trajectory\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Density Matrix Visualization\n",
    "\n",
    "Visualize predicted vs true density matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select timestep to visualize\n",
    "if results:\n",
    "    result = results[0]\n",
    "    n_steps = len(result['pred'])\n",
    "    \n",
    "    # Visualize at different timesteps\n",
    "    timesteps = [0, n_steps // 4, n_steps // 2, 3 * n_steps // 4, n_steps - 1]\n",
    "    \n",
    "    print(f\"Trajectory length: {n_steps}\")\n",
    "    print(f\"Visualizing timesteps: {timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density comparison at selected timestep\n",
    "if results:\n",
    "    t = n_steps // 2  # Middle of trajectory\n",
    "    \n",
    "    fig = plot_density_comparison(\n",
    "        rho_pred=result['pred'][t],\n",
    "        rho_true=result['true'][t],\n",
    "        component='abs',\n",
    "        title=f\"Density Matrix at t={t} (|rho|)\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot real and imaginary parts\n",
    "if results:\n",
    "    t = n_steps // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Real part\n",
    "    for idx, (ax, data, title) in enumerate([\n",
    "        (axes[0, 0], result['pred'][t].real.cpu().numpy(), 'Predicted (Real)'),\n",
    "        (axes[0, 1], result['true'][t].real.cpu().numpy(), 'True (Real)'),\n",
    "        (axes[0, 2], (result['pred'][t] - result['true'][t]).real.cpu().numpy(), 'Diff (Real)'),\n",
    "    ]):\n",
    "        vmax = max(np.abs(result['pred'][t].real.cpu().numpy()).max(),\n",
    "                   np.abs(result['true'][t].real.cpu().numpy()).max())\n",
    "        if 'Diff' in title:\n",
    "            vmax = np.abs(data).max()\n",
    "        im = ax.imshow(data, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "        ax.set_title(title)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Imaginary part\n",
    "    for idx, (ax, data, title) in enumerate([\n",
    "        (axes[1, 0], result['pred'][t].imag.cpu().numpy(), 'Predicted (Imag)'),\n",
    "        (axes[1, 1], result['true'][t].imag.cpu().numpy(), 'True (Imag)'),\n",
    "        (axes[1, 2], (result['pred'][t] - result['true'][t]).imag.cpu().numpy(), 'Diff (Imag)'),\n",
    "    ]):\n",
    "        vmax = max(np.abs(result['pred'][t].imag.cpu().numpy()).max(),\n",
    "                   np.abs(result['true'][t].imag.cpu().numpy()).max())\n",
    "        if 'Diff' in title:\n",
    "            vmax = np.abs(data).max()\n",
    "        im = ax.imshow(data, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "        ax.set_title(title)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.suptitle(f'Density Matrix Comparison at t={t}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate density evolution (create frames)\n",
    "if results:\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    # Get data ranges for consistent colorbar\n",
    "    vmax_pred = np.abs(result['pred'].cpu().numpy()).max()\n",
    "    vmax_true = np.abs(result['true'].cpu().numpy()).max()\n",
    "    vmax = max(vmax_pred, vmax_true)\n",
    "    \n",
    "    ims = []\n",
    "    for ax in axes:\n",
    "        im = ax.imshow(np.zeros_like(result['pred'][0].abs().cpu().numpy()), \n",
    "                       cmap='viridis', vmin=0, vmax=vmax)\n",
    "        ims.append(im)\n",
    "    \n",
    "    axes[0].set_title('Predicted |rho|')\n",
    "    axes[1].set_title('True |rho|')\n",
    "    axes[2].set_title('Difference')\n",
    "    \n",
    "    def update(frame):\n",
    "        pred = result['pred'][frame].abs().cpu().numpy()\n",
    "        true = result['true'][frame].abs().cpu().numpy()\n",
    "        diff = np.abs(pred - true)\n",
    "        \n",
    "        ims[0].set_array(pred)\n",
    "        ims[1].set_array(true)\n",
    "        ims[2].set_array(diff)\n",
    "        fig.suptitle(f't = {frame}')\n",
    "        return ims\n",
    "    \n",
    "    # Create animation (subsample for speed)\n",
    "    frames = range(0, len(result['pred']), max(1, len(result['pred']) // 50))\n",
    "    anim = FuncAnimation(fig, update, frames=frames, interval=100, blit=True)\n",
    "    \n",
    "    plt.close()  # Don't show static figure\n",
    "    HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Comparison\n",
    "\n",
    "Compare metrics across different models or checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoints to compare\n",
    "checkpoints_to_compare = {\n",
    "    # 'Phase 1': '../checkpoints/phase1_h2p/best.pt',\n",
    "    # 'Phase 2': '../checkpoints/phase2_multi_mol/best.pt',\n",
    "    # 'Phase 3': '../checkpoints/phase3_generalization/best.pt',\n",
    "}\n",
    "\n",
    "if not checkpoints_to_compare:\n",
    "    print(\"Add checkpoint paths to compare models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each checkpoint and collect metrics\n",
    "comparison_metrics = {}\n",
    "\n",
    "for name, ckpt_path in checkpoints_to_compare.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    model = load_model(ckpt_path, device=device)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    acc = MetricsAccumulator()\n",
    "    \n",
    "    for traj in trajectories[:5]:  # Use subset for speed\n",
    "        eval_result = evaluate_trajectory(\n",
    "            model, traj, max_steps=MAX_ROLLOUT_STEPS, device=device\n",
    "        )\n",
    "        metrics = compute_trajectory_metrics(\n",
    "            trajectory_pred=eval_result['pred'],\n",
    "            trajectory_true=eval_result['true'],\n",
    "            overlap=eval_result['overlap'],\n",
    "            n_electrons=eval_result['n_electrons'],\n",
    "        )\n",
    "        acc.add({\n",
    "            'mse': metrics.mse,\n",
    "            'mae': metrics.mae,\n",
    "            'relative_error': metrics.relative_error,\n",
    "        })\n",
    "    \n",
    "    summary = acc.compute_summary()\n",
    "    comparison_metrics[name] = {k: v['mean'] for k, v in summary.items()}\n",
    "    print(f\"  MSE: {comparison_metrics[name]['mse']:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "if comparison_metrics:\n",
    "    fig = plot_metrics_comparison(\n",
    "        metrics_dict=comparison_metrics,\n",
    "        title=\"Model Comparison\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results\n",
    "\n",
    "Save results to JSON for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for export\n",
    "if results:\n",
    "    export_data = {\n",
    "        'config': {\n",
    "            'checkpoint': str(CHECKPOINT_PATH),\n",
    "            'data_path': str(DATA_PATH),\n",
    "            'molecule': MOLECULE,\n",
    "            'max_rollout_steps': MAX_ROLLOUT_STEPS,\n",
    "            'dt': DT,\n",
    "        },\n",
    "        'summary': accumulator.compute_summary(),\n",
    "        'n_trajectories': len(results),\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = Path('../results') / f'{MOLECULE}_evaluation.json'\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all figures\n",
    "close_all()\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
