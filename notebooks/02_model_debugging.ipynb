{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-TDDFT Model Debugging\n",
    "\n",
    "This notebook provides tools for debugging the RT-TDDFT ML model:\n",
    "\n",
    "1. **Model Architecture** - Inspect layers, parameters, shapes\n",
    "2. **Forward Pass** - Step-by-step execution with hooks\n",
    "3. **Gradient Analysis** - Gradient flow, vanishing/exploding gradients\n",
    "4. **Activation Statistics** - Layer activations, dead neurons\n",
    "5. **Weight Analysis** - Distributions, initialization quality\n",
    "6. **Shape Debugging** - Input/output dimensions at each layer\n",
    "7. **Memory Profiling** - GPU memory usage\n",
    "8. **Common Issues** - NaN detection, numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.models import (\n",
    "    RTTDDFTModel,\n",
    "    GeometryEncoder,\n",
    "    DensityEncoder,\n",
    "    FieldEncoder,\n",
    "    GeometryConditionedMamba,\n",
    "    DensityDecoder,\n",
    ")\n",
    "from src.utils import build_molecular_graph, close_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'latent_dim': 256,\n",
    "    'n_mamba_layers': 6,\n",
    "    'd_state': 16,\n",
    "    'geometry_irreps': '32x0e + 16x1o + 8x2e',\n",
    "    'max_ell': 2,\n",
    "}\n",
    "\n",
    "# Test data configuration\n",
    "TEST_CONFIG = {\n",
    "    'batch_size': 2,\n",
    "    'n_atoms': 3,\n",
    "    'n_basis': 10,\n",
    "    'seq_length': 16,\n",
    "    'n_electrons': 4,\n",
    "}\n",
    "\n",
    "# Checkpoint path (optional)\n",
    "CHECKPOINT_PATH = None  # Set to load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Model Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def format_params(n):\n",
    "    \"\"\"Format parameter count.\"\"\"\n",
    "    if n >= 1e6:\n",
    "        return f\"{n/1e6:.2f}M\"\n",
    "    elif n >= 1e3:\n",
    "        return f\"{n/1e3:.1f}K\"\n",
    "    return str(n)\n",
    "\n",
    "def print_model_summary(model, indent=0):\n",
    "    \"\"\"Print hierarchical model summary.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    total, trainable = count_parameters(model)\n",
    "    \n",
    "    print(f\"{prefix}{model.__class__.__name__}: {format_params(trainable)} params\")\n",
    "    \n",
    "    for name, child in model.named_children():\n",
    "        child_total, child_trainable = count_parameters(child)\n",
    "        print(f\"{prefix}  └─ {name}: {format_params(child_trainable)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load model\n",
    "def create_model(config, checkpoint_path=None):\n",
    "    \"\"\"Create model from config or load from checkpoint.\"\"\"\n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        print(f\"Loading model from {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "        if 'model' in ckpt:\n",
    "            return ckpt['model']\n",
    "        else:\n",
    "            raise ValueError(\"Checkpoint doesn't contain full model\")\n",
    "    \n",
    "    print(\"Creating new model from config\")\n",
    "    model = RTTDDFTModel(\n",
    "        latent_dim=config['latent_dim'],\n",
    "        n_mamba_layers=config['n_mamba_layers'],\n",
    "        d_state=config['d_state'],\n",
    "        geometry_irreps=config['geometry_irreps'],\n",
    "        max_ell=config['max_ell'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "try:\n",
    "    model = create_model(MODEL_CONFIG, CHECKPOINT_PATH)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    print(\"\\nCreating minimal mock model for demonstration...\")\n",
    "    \n",
    "    class MockModel(nn.Module):\n",
    "        def __init__(self, latent_dim=256, n_basis=10):\n",
    "            super().__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(n_basis * n_basis * 2, latent_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(latent_dim, latent_dim),\n",
    "            )\n",
    "            self.dynamics = nn.LSTM(latent_dim + 3, latent_dim, num_layers=2, batch_first=True)\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, latent_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(latent_dim, n_basis * n_basis * 2),\n",
    "            )\n",
    "            self.n_basis = n_basis\n",
    "            self.latent_dim = latent_dim\n",
    "        \n",
    "        def forward(self, batch):\n",
    "            rho = batch['density']  # (B, n, n) complex\n",
    "            field = batch['field']  # (B, 3)\n",
    "            \n",
    "            B = rho.shape[0]\n",
    "            n = self.n_basis\n",
    "            \n",
    "            # Flatten and encode\n",
    "            rho_flat = torch.cat([rho.real.reshape(B, -1), rho.imag.reshape(B, -1)], dim=-1)\n",
    "            z = self.encoder(rho_flat)\n",
    "            \n",
    "            # Combine with field\n",
    "            z_field = torch.cat([z, field], dim=-1).unsqueeze(1)\n",
    "            h, _ = self.dynamics(z_field)\n",
    "            h = h.squeeze(1)\n",
    "            \n",
    "            # Decode\n",
    "            out = self.decoder(h)\n",
    "            rho_real = out[:, :n*n].reshape(B, n, n)\n",
    "            rho_imag = out[:, n*n:].reshape(B, n, n)\n",
    "            rho_out = rho_real + 1j * rho_imag\n",
    "            \n",
    "            # Make Hermitian\n",
    "            rho_out = 0.5 * (rho_out + rho_out.conj().transpose(-2, -1))\n",
    "            return rho_out\n",
    "    \n",
    "    model = MockModel(latent_dim=256, n_basis=TEST_CONFIG['n_basis']).to(device)\n",
    "    print(\"Created mock model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"\\nTotal parameters: {format_params(total)} ({total:,})\")\n",
    "print(f\"Trainable parameters: {format_params(trainable)} ({trainable:,})\")\n",
    "\n",
    "print(\"\\nModule hierarchy:\")\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed layer breakdown\n",
    "print(\"\\nDetailed Layer Breakdown:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Layer':<40} {'Shape':<20} {'Params':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:<40} {str(list(param.shape)):<20} {param.numel():>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distribution by module\n",
    "module_params = defaultdict(int)\n",
    "for name, param in model.named_parameters():\n",
    "    module = name.split('.')[0]\n",
    "    module_params[module] += param.numel()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "modules = list(module_params.keys())\n",
    "params = list(module_params.values())\n",
    "plt.barh(modules, params)\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.title('Parameters by Module')\n",
    "for i, v in enumerate(params):\n",
    "    plt.text(v + max(params)*0.01, i, format_params(v), va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Create Test Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_batch(config, device='cpu'):\n",
    "    \"\"\"Create a test batch for model debugging.\"\"\"\n",
    "    B = config['batch_size']\n",
    "    n = config['n_basis']\n",
    "    n_atoms = config['n_atoms']\n",
    "    n_elec = config['n_electrons']\n",
    "    \n",
    "    # Create Hermitian density matrix with correct trace\n",
    "    rho = torch.randn(B, n, n, dtype=torch.complex64, device=device)\n",
    "    rho = 0.5 * (rho + rho.conj().transpose(-2, -1))  # Hermitianize\n",
    "    \n",
    "    # Normalize trace\n",
    "    overlap = torch.eye(n, dtype=torch.complex64, device=device)\n",
    "    for i in range(B):\n",
    "        trace = torch.einsum('ij,ji->', rho[i], overlap).real\n",
    "        rho[i] = rho[i] * (n_elec / trace)\n",
    "    \n",
    "    # External field\n",
    "    field = 0.01 * torch.randn(B, 3, device=device)\n",
    "    \n",
    "    # Geometry (simple linear molecule)\n",
    "    positions = torch.zeros(B, n_atoms, 3, device=device)\n",
    "    for i in range(n_atoms):\n",
    "        positions[:, i, 0] = i * 2.0  # 2 Bohr spacing\n",
    "    \n",
    "    atomic_numbers = torch.ones(B, n_atoms, dtype=torch.long, device=device)\n",
    "    \n",
    "    batch = {\n",
    "        'density': rho,\n",
    "        'field': field,\n",
    "        'positions': positions,\n",
    "        'atomic_numbers': atomic_numbers,\n",
    "        'overlap': overlap.unsqueeze(0).expand(B, -1, -1),\n",
    "        'n_electrons': torch.tensor([n_elec] * B, device=device),\n",
    "    }\n",
    "    \n",
    "    return batch\n",
    "\n",
    "test_batch = create_test_batch(TEST_CONFIG, device=device)\n",
    "\n",
    "print(\"Test batch created:\")\n",
    "for key, val in test_batch.items():\n",
    "    if isinstance(val, torch.Tensor):\n",
    "        print(f\"  {key}: {val.shape} {val.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Forward Pass Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardHook:\n",
    "    \"\"\"Hook to capture layer activations.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.activations = OrderedDict()\n",
    "        self.handles = []\n",
    "    \n",
    "    def hook_fn(self, name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]  # Handle LSTM/GRU outputs\n",
    "            self.activations[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def register(self, model):\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules\n",
    "                handle = module.register_forward_hook(self.hook_fn(name))\n",
    "                self.handles.append(handle)\n",
    "    \n",
    "    def remove(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.handles = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.activations = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass with hooks\n",
    "forward_hook = ForwardHook()\n",
    "forward_hook.register(model)\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        output = model(test_batch)\n",
    "    print(\"Forward pass successful!\")\n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Output dtype: {output.dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"Forward pass failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "forward_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze layer activations\n",
    "print(\"\\nLayer Activations:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Layer':<40} {'Shape':<20} {'Mean':>10} {'Std':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, activation in forward_hook.activations.items():\n",
    "    if activation.numel() > 0:\n",
    "        if activation.is_complex():\n",
    "            act_real = activation.real.float()\n",
    "        else:\n",
    "            act_real = activation.float()\n",
    "        mean = act_real.mean().item()\n",
    "        std = act_real.std().item()\n",
    "        shape = str(list(activation.shape))\n",
    "        print(f\"{name[:40]:<40} {shape:<20} {mean:>10.4f} {std:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation distributions for select layers\n",
    "interesting_layers = [name for name in forward_hook.activations.keys() \n",
    "                      if any(x in name for x in ['linear', 'Linear', 'conv', 'dense'])]\n",
    "\n",
    "if interesting_layers:\n",
    "    n_plots = min(len(interesting_layers), 6)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, name in enumerate(interesting_layers[:n_plots]):\n",
    "        activation = forward_hook.activations[name]\n",
    "        if activation.is_complex():\n",
    "            data = activation.real.cpu().numpy().flatten()\n",
    "        else:\n",
    "            data = activation.cpu().numpy().flatten()\n",
    "        \n",
    "        axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(name.split('.')[-1][:20])\n",
    "        axes[idx].set_xlabel('Activation')\n",
    "        axes[idx].set_ylabel('Count')\n",
    "    \n",
    "    for idx in range(n_plots, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Activation Distributions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No linear/conv layers found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Gradient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientHook:\n",
    "    \"\"\"Hook to capture gradients during backward pass.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradients = OrderedDict()\n",
    "        self.handles = []\n",
    "    \n",
    "    def hook_fn(self, name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            if grad_output[0] is not None:\n",
    "                self.gradients[name] = grad_output[0].detach()\n",
    "        return hook\n",
    "    \n",
    "    def register(self, model):\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:\n",
    "                handle = module.register_full_backward_hook(self.hook_fn(name))\n",
    "                self.handles.append(handle)\n",
    "    \n",
    "    def remove(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "model.train()\n",
    "gradient_hook = GradientHook()\n",
    "gradient_hook.register(model)\n",
    "\n",
    "try:\n",
    "    # Forward\n",
    "    output = model(test_batch)\n",
    "    \n",
    "    # Simple loss (MSE to target)\n",
    "    target = test_batch['density']\n",
    "    loss = (output - target).abs().pow(2).mean()\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.6f}\")\n",
    "    print(\"Backward pass successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Backward pass failed: {e}\")\n",
    "\n",
    "gradient_hook.remove()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter gradients\n",
    "print(\"\\nParameter Gradients:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Parameter':<40} {'Grad Mean':>12} {'Grad Std':>12} {'Grad Max':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "grad_stats = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad = param.grad.float()\n",
    "        stats = {\n",
    "            'name': name,\n",
    "            'mean': grad.abs().mean().item(),\n",
    "            'std': grad.std().item(),\n",
    "            'max': grad.abs().max().item(),\n",
    "        }\n",
    "        grad_stats.append(stats)\n",
    "        print(f\"{name[:40]:<40} {stats['mean']:>12.2e} {stats['std']:>12.2e} {stats['max']:>12.2e}\")\n",
    "\n",
    "# Zero gradients for next experiment\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for vanishing/exploding gradients\n",
    "if grad_stats:\n",
    "    means = [s['mean'] for s in grad_stats]\n",
    "    \n",
    "    print(\"\\nGradient Health Check:\")\n",
    "    print(f\"  Overall gradient magnitude: {np.mean(means):.2e}\")\n",
    "    \n",
    "    vanishing = sum(1 for m in means if m < 1e-7)\n",
    "    exploding = sum(1 for m in means if m > 1e3)\n",
    "    \n",
    "    print(f\"  Vanishing gradients (<1e-7): {vanishing}/{len(means)} layers\")\n",
    "    print(f\"  Exploding gradients (>1e3): {exploding}/{len(means)} layers\")\n",
    "    \n",
    "    if vanishing > len(means) // 2:\n",
    "        print(\"  WARNING: Many vanishing gradients detected!\")\n",
    "    if exploding > 0:\n",
    "        print(\"  WARNING: Exploding gradients detected!\")\n",
    "    if vanishing == 0 and exploding == 0:\n",
    "        print(\"  OK: Gradient magnitudes look healthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gradient flow\n",
    "if grad_stats:\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    names = [s['name'].split('.')[-1][:15] for s in grad_stats]\n",
    "    means = [s['mean'] for s in grad_stats]\n",
    "    \n",
    "    plt.bar(range(len(means)), means)\n",
    "    plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Gradient Magnitude (log)')\n",
    "    plt.title('Gradient Flow Through Network')\n",
    "    plt.axhline(y=1e-7, color='r', linestyle='--', label='Vanishing threshold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight distributions\n",
    "print(\"Weight Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Parameter':<40} {'Mean':>12} {'Std':>12} {'Min':>12} {'Max':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "weight_stats = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        w = param.data.float()\n",
    "        stats = {\n",
    "            'name': name,\n",
    "            'mean': w.mean().item(),\n",
    "            'std': w.std().item(),\n",
    "            'min': w.min().item(),\n",
    "            'max': w.max().item(),\n",
    "        }\n",
    "        weight_stats.append(stats)\n",
    "        print(f\"{name[:40]:<40} {stats['mean']:>12.4f} {stats['std']:>12.4f} \"\n",
    "              f\"{stats['min']:>12.4f} {stats['max']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight distributions\n",
    "weight_params = [(name, param) for name, param in model.named_parameters() if 'weight' in name]\n",
    "\n",
    "if weight_params:\n",
    "    n_plots = min(len(weight_params), 6)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, param) in enumerate(weight_params[:n_plots]):\n",
    "        data = param.data.cpu().float().numpy().flatten()\n",
    "        axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(name.split('.')[-2] + '.' + name.split('.')[-1])\n",
    "        axes[idx].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    for idx in range(n_plots, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Weight Distributions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initialization quality\n",
    "print(\"\\nInitialization Quality Check:\")\n",
    "for stats in weight_stats:\n",
    "    name = stats['name']\n",
    "    \n",
    "    # Check for common issues\n",
    "    issues = []\n",
    "    \n",
    "    if abs(stats['mean']) > 0.1:\n",
    "        issues.append(f\"mean={stats['mean']:.3f} (should be ~0)\")\n",
    "    \n",
    "    if stats['std'] < 0.01:\n",
    "        issues.append(f\"std={stats['std']:.4f} (too small)\")\n",
    "    elif stats['std'] > 2.0:\n",
    "        issues.append(f\"std={stats['std']:.3f} (too large)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  {name[:50]}: {', '.join(issues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Shape Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_shapes(model, batch):\n",
    "    \"\"\"Trace tensor shapes through the network.\"\"\"\n",
    "    shapes = OrderedDict()\n",
    "    handles = []\n",
    "    \n",
    "    def hook(name):\n",
    "        def fn(module, input, output):\n",
    "            input_shapes = [tuple(x.shape) if isinstance(x, torch.Tensor) else type(x) for x in input]\n",
    "            if isinstance(output, tuple):\n",
    "                output_shapes = [tuple(x.shape) if isinstance(x, torch.Tensor) else type(x) for x in output]\n",
    "            elif isinstance(output, torch.Tensor):\n",
    "                output_shapes = tuple(output.shape)\n",
    "            else:\n",
    "                output_shapes = type(output)\n",
    "            shapes[name] = {'input': input_shapes, 'output': output_shapes}\n",
    "        return fn\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if name:  # Skip root module\n",
    "            handles.append(module.register_forward_hook(hook(name)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(batch)\n",
    "    \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace shapes\n",
    "shape_trace = trace_shapes(model, test_batch)\n",
    "\n",
    "print(\"Shape Trace:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Module':<40} {'Input Shape':<30} {'Output Shape':<30}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for name, shapes in shape_trace.items():\n",
    "    input_str = str(shapes['input'])[:28]\n",
    "    output_str = str(shapes['output'])[:28]\n",
    "    print(f\"{name[:40]:<40} {input_str:<30} {output_str:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"Get GPU memory statistics.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "        'reserved': torch.cuda.memory_reserved() / 1e9,\n",
    "        'max_allocated': torch.cuda.max_memory_allocated() / 1e9,\n",
    "    }\n",
    "\n",
    "def print_memory_stats(label=\"\"):\n",
    "    \"\"\"Print memory statistics.\"\"\"\n",
    "    stats = get_memory_stats()\n",
    "    if stats:\n",
    "        print(f\"{label} - Allocated: {stats['allocated']:.2f} GB, \"\n",
    "              f\"Reserved: {stats['reserved']:.2f} GB, \"\n",
    "              f\"Max: {stats['max_allocated']:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"{label} - GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Reset memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Memory Profiling:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print_memory_stats(\"Before forward\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(test_batch)\n",
    "    \n",
    "    print_memory_stats(\"After forward\")\n",
    "    \n",
    "    # With gradients\n",
    "    model.train()\n",
    "    output = model(test_batch)\n",
    "    loss = output.abs().pow(2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    print_memory_stats(\"After backward\")\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    \n",
    "    # Estimate memory per batch element\n",
    "    mem_per_sample = torch.cuda.max_memory_allocated() / 1e6 / TEST_CONFIG['batch_size']\n",
    "    print(f\"\\nEstimated memory per sample: {mem_per_sample:.1f} MB\")\n",
    "    \n",
    "    # Estimate max batch size\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    usable_mem = total_mem * 0.8  # Leave 20% headroom\n",
    "    max_batch = int(usable_mem * 1e3 / mem_per_sample)\n",
    "    print(f\"Estimated max batch size: {max_batch}\")\n",
    "else:\n",
    "    print(\"GPU not available for memory profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Numerical Stability Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numerical_stability(model, batch, n_iters=10):\n",
    "    \"\"\"Check for NaN/Inf in forward passes.\"\"\"\n",
    "    model.eval()\n",
    "    issues = []\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        with torch.no_grad():\n",
    "            output = model(batch)\n",
    "        \n",
    "        if torch.isnan(output).any():\n",
    "            issues.append(f\"Iteration {i}: NaN in output\")\n",
    "        if torch.isinf(output).any():\n",
    "            issues.append(f\"Iteration {i}: Inf in output\")\n",
    "        \n",
    "        # Use output as next input (simulate autoregressive)\n",
    "        batch = batch.copy()\n",
    "        batch['density'] = output\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical Stability Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check with normal inputs\n",
    "issues = check_numerical_stability(model, test_batch)\n",
    "if issues:\n",
    "    print(\"ISSUES FOUND:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(\"OK: No NaN/Inf detected in 10 iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with edge case inputs\n",
    "print(\"\\nEdge Case Testing:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "edge_cases = [\n",
    "    ('Zero density', torch.zeros_like(test_batch['density'])),\n",
    "    ('Large density', test_batch['density'] * 100),\n",
    "    ('Small density', test_batch['density'] * 0.001),\n",
    "    ('Identity density', torch.eye(TEST_CONFIG['n_basis'], dtype=torch.complex64, device=device).unsqueeze(0).expand(TEST_CONFIG['batch_size'], -1, -1)),\n",
    "]\n",
    "\n",
    "for name, density in edge_cases:\n",
    "    edge_batch = test_batch.copy()\n",
    "    edge_batch['density'] = density\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output = model(edge_batch)\n",
    "        \n",
    "        has_nan = torch.isnan(output).any().item()\n",
    "        has_inf = torch.isinf(output).any().item()\n",
    "        \n",
    "        if has_nan or has_inf:\n",
    "            print(f\"  {name}: FAIL (NaN={has_nan}, Inf={has_inf})\")\n",
    "        else:\n",
    "            print(f\"  {name}: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: ERROR - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output physics\n",
    "print(\"\\nOutput Physics Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_batch)\n",
    "\n",
    "# Check Hermiticity\n",
    "herm_error = (output - output.conj().transpose(-2, -1)).abs().max().item()\n",
    "print(f\"  Hermiticity error: {herm_error:.2e}\")\n",
    "\n",
    "# Check trace\n",
    "overlap = test_batch['overlap'][0]\n",
    "traces = []\n",
    "for i in range(output.shape[0]):\n",
    "    trace = torch.einsum('ij,ji->', output[i], overlap).real.item()\n",
    "    traces.append(trace)\n",
    "print(f\"  Trace values: {traces}\")\n",
    "print(f\"  Expected: {TEST_CONFIG['n_electrons']}\")\n",
    "\n",
    "# Check magnitude\n",
    "max_mag = output.abs().max().item()\n",
    "print(f\"  Max magnitude: {max_mag:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_forward(model, batch, n_warmup=5, n_runs=20):\n",
    "    \"\"\"Benchmark forward pass.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(n_warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model(batch)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model(batch)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return np.array(times) * 1000  # Convert to ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance Benchmark:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "times = benchmark_forward(model, test_batch)\n",
    "\n",
    "print(f\"Forward pass time:\")\n",
    "print(f\"  Mean: {times.mean():.2f} ms\")\n",
    "print(f\"  Std:  {times.std():.2f} ms\")\n",
    "print(f\"  Min:  {times.min():.2f} ms\")\n",
    "print(f\"  Max:  {times.max():.2f} ms\")\n",
    "\n",
    "throughput = TEST_CONFIG['batch_size'] / (times.mean() / 1000)\n",
    "print(f\"\\nThroughput: {throughput:.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "throughputs = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    try:\n",
    "        config = TEST_CONFIG.copy()\n",
    "        config['batch_size'] = bs\n",
    "        batch = create_test_batch(config, device=device)\n",
    "        \n",
    "        times = benchmark_forward(model, batch, n_warmup=3, n_runs=10)\n",
    "        throughput = bs / (times.mean() / 1000)\n",
    "        throughputs.append(throughput)\n",
    "        print(f\"Batch size {bs}: {times.mean():.2f} ms, {throughput:.1f} samples/sec\")\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e).lower():\n",
    "            print(f\"Batch size {bs}: OOM\")\n",
    "            throughputs.append(0)\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot throughput vs batch size\n",
    "if throughputs:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    valid_idx = [i for i, t in enumerate(throughputs) if t > 0]\n",
    "    plt.plot([batch_sizes[i] for i in valid_idx], \n",
    "             [throughputs[i] for i in valid_idx], \n",
    "             'o-', markersize=8)\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Throughput (samples/sec)')\n",
    "    plt.title('Throughput vs Batch Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DEBUGGING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  Parameters: {format_params(count_parameters(model)[1])}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(f\"\\nForward Pass:\")\n",
    "print(f\"  Status: {'OK' if 'output' in dir() else 'FAILED'}\")\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "if grad_stats:\n",
    "    vanishing = sum(1 for s in grad_stats if s['mean'] < 1e-7)\n",
    "    print(f\"  Vanishing: {vanishing}/{len(grad_stats)} layers\")\n",
    "else:\n",
    "    print(f\"  Not computed\")\n",
    "\n",
    "print(f\"\\nNumerical Stability:\")\n",
    "print(f\"  Issues: {len(issues) if 'issues' in dir() else 'Not checked'}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "if 'times' in dir():\n",
    "    print(f\"  Forward: {times.mean():.2f} ms\")\n",
    "    print(f\"  Throughput: {throughput:.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "close_all()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\nDebugging session complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
